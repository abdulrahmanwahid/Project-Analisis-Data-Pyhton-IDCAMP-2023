# -*- coding: utf-8 -*-
"""Notebook Data Analyst Dicoding.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vkgun0iuxuUZFKu0OVLL3qN_KE797_wE

# **E-Commerce Public Data Analysis Project**

Belajar Analisis Data dengan Python

Proyek Analisis Data: E-Commerce Public Dataset

Name : Abdul Rahman Wahid
Email : wahidabdul2801@gmail.com
Username : abdulrwahid

This is my submission on IDCAMP 2023 Data Scientist Path.

For all viewers: Don't plagiarize my code, I will hide and delete my github accountâ˜¹

## **Menentukan Pertanyaan Bisnis**

1. Bagaimana demografi pelanggan yang kita miliki?
2. Produk apa yang paling banyak dan paling sedikit terjual?
3. Bagaimana performa penjualan dan revenue perusahaan dalam setiap bulan (2016-09 sampai 2018-08) ?

## **Menyiapkan Library**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import urllib.request
import seaborn as sns

"""## **Data Wrangling**

### **Gathering Data**
"""

customers_df = pd.read_csv("data/customers_dataset.csv")
customers_df.head()

geolocation_df = pd.read_csv("data/geolocation_dataset.csv")
geolocation_df.head()

order_items_df = pd.read_csv("data/order_items_dataset.csv")
order_items_df.head()

order_payments_df = pd.read_csv("data/order_payments_dataset.csv")
order_payments_df.head()

order_reviews_df = pd.read_csv("data/order_reviews_dataset.csv")
order_reviews_df.head()

orders_df = pd.read_csv("data/orders_dataset.csv")
orders_df.head()

product_category_name_df = pd.read_csv("data/product_category_name_translation.csv")
product_category_name_df.head()

products_df = pd.read_csv("data/products_dataset.csv")
products_df.head()

sellers_df = pd.read_csv("data/sellers_dataset.csv")
sellers_df.head()

"""### **Assessing Data**

#### Menilai tabel `customers_df`
"""

customers_df.info()

customers_df.isna().sum()

print("Jumlah duplikasi: ", customers_df.duplicated().sum())

customers_df.describe()

"""#### Menilai tabel `geolocation_df`"""

geolocation_df.info()

geolocation_df.isna().sum()

print("Jumlah duplikasi: ", geolocation_df.duplicated().sum())

geolocation_df.describe()

"""#### Menilai tabel `order_items_df`"""

order_items_df.info()

order_items_df.isna().sum()

print("Jumlah duplikasi: ", order_items_df.duplicated().sum())

order_items_df.describe()

"""#### Menilai tabel `order_payments_df`"""

order_payments_df.info()

order_payments_df.isna().sum()

print("Jumlah duplikasi: ", order_payments_df.duplicated().sum())

order_payments_df.describe()

"""#### Menilai tabel `order_reviews_df`"""

order_reviews_df.info()

order_reviews_df.isna().sum()

print("Jumlah duplikasi: ", order_reviews_df.duplicated().sum())

order_reviews_df.describe()

"""#### Menilai tabel `orders_df`"""

orders_df.info()

orders_df.isna().sum()

print("Jumlah duplikasi: ", orders_df.duplicated().sum())

orders_df.describe()

"""#### Menilai tabel `product_category_name_df`"""

product_category_name_df.info()

product_category_name_df.isna().sum()

print("Jumlah duplikasi: ", product_category_name_df.duplicated().sum())

product_category_name_df.describe()

"""#### Menilai tabel `products_df`"""

products_df.info()

products_df.isna().sum()

print("Jumlah duplikasi: ", products_df.duplicated().sum())

products_df.describe()

"""#### Menilai tabel `sellers_df`"""

sellers_df.info()

sellers_df.isna().sum()

print("Jumlah duplikasi: ", sellers_df.duplicated().sum())

sellers_df.describe()

"""### **Cleaning Data**

Berdasarkan proses **Assessing Data**, beberapa hal yang perlu dilakukan proses **Cleaning data** yaitu :

#### **`order_items_df`**

Mengganti tipe data pada kolom `shiping_limit_date` menjadi `datetime`

#### **`order_reviews_df`**

1. Mengganti tipe data pada kolom `review_creation_date` dan `review_answer_timestamp` menjadi `datetime`

2. Karena banyak sekali missing value pada kolom `review_comment_title` dan `review_comment_message`, alih-alih menghapus semua missing value tersebut, saya melakukan proses `imputation` agar data yang kosong pada kedua kolom tersebut dapat terisi berdasarkan value dari `review_score`

#### **`orders_df`**

1. Mengganti tipe data kolom `order_purchase_timestamp`, `order_approved_at`, `order_delivered_carrier_date`, `order_delivered_customer_date`, dan `order_estimated_delivery_date` menjadi `datetime`

2. Karena missing value yang ada pada tabel tersebut terdapat dalam kolom `order_approved_at`, `order_delivered_carrier_date`, dan `order_delivered_customer_date` memiliki tipe data `datetime` (berkaitan dengan waktu), maka disini saya memutuskan untuk melakukan dropping missing value pada ketiga kolom tersebut karena alasan penggunaan metode imputation atau interpolation yang tidak memungkinkan.

#### **`products_df`**

Melakukan dropping pada semua kolom yang memiliki `missing value`. Alasan tersebut didasari karena jumlah data dalam kolom yang miliki missing value relatif sedikit jika dibandingkan dengan kesuluruhan data.

#### **`geolocation_df`**

Melakukan `proses reduksi` untuk menghasilkan DataFrame yang lebih ringkas yang nantinya akan digunakan dalam analisis data. Proses reduksi tersebut dilakukan pada data yang dianggap redundan atau tidak memberikan nilai tambah signifikan untuk analisis yang sedang dilakukan.

#### **1. Membersihkan tabel `order_items_df`**

Mengganti tipe data pada kolom `shiping_limit_date` menjadi `datetime`
"""

order_items_df['shipping_limit_date'] = pd.to_datetime(order_items_df['shipping_limit_date'])

"""Melakukan pengecekan untuk memastikan bahwa tipe data dari kolom `shiping_limit_date` sudah berubah menjadi `datetime`"""

order_items_df.info()

"""#### **2. Membersihkan tabel `order_reviews_df`**

1. Mengganti tipe data pada kolom `review_creation_date` dan `review_answer_timestamp` menjadi `datetime`
"""

order_reviews_df['review_creation_date'] = pd.to_datetime(order_reviews_df['review_creation_date'])
order_reviews_df['review_answer_timestamp'] = pd.to_datetime(order_reviews_df['review_answer_timestamp'])

"""Melakukan pengecekan untuk memastikan bahwa tipe data dari kolom `review_creation_date` dan `review_answer_timestamp` sudah berubah menjadi `datetime`"""

order_reviews_df.info()

"""2. Karena banyak sekali missing value pada kolom `review_comment_title` dan `review_comment_message`, alih-alih menghapus semua missing value tersebut, saya melakukan proses `imputation` agar data yang kosong pada kedua kolom tersebut dapat terisi berdasarkan value dari `review_score`"""

score_mapping = {1: 'very bad', 2: 'bad', 3: 'neutral', 4: 'good', 5: 'very good'}

# Mengisi nilai null pada kolom review_comment_title dan review_comment_message berdasarkan review_score
order_reviews_df['review_comment_title'] = order_reviews_df.apply(lambda row: score_mapping[row['review_score']] if pd.isnull(row['review_comment_title']) else row['review_comment_title'], axis=1)
order_reviews_df['review_comment_message'] = order_reviews_df.apply(lambda row: score_mapping[row['review_score']] if pd.isnull(row['review_comment_message']) else row['review_comment_message'], axis=1)

"""Melakukan pengecekan untuk memastikan bahwa missing value pada kolom `review_comment_title` dan `review_comment_message` sudah berhasil diisi"""

order_reviews_df.isna().sum()

order_reviews_df.head()

"""#### **3. Membersihkan tabel `orders_df`**

1. Mengganti tipe data kolom `order_purchase_timestamp`, `order_approved_at`, `order_delivered_carrier_date`, `order_delivered_customer_date`, dan `order_estimated_delivery_date` menjadi `datetime`
"""

date_columns = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']
orders_df[date_columns] = orders_df[date_columns].apply(pd.to_datetime)

"""Melakukan pengecekan untuk memastikan bahwa tipe data pada beberapa kolom sudah berhasil diganti menjadi datetime"""

orders_df.info()

"""2. Karena missing value yang ada pada tabel tersebut terdapat dalam kolom `order_approved_at`, `order_delivered_carrier_date`, dan `order_delivered_customer_date` memiliki tipe data `datetime` (berkaitan dengan waktu), maka disini saya memutuskan untuk melakukan dropping missing value pada ketiga kolom tersebut karena alasan penggunaan metode imputation atau interpolation yang tidak memungkinkan."""

orders_df.dropna(axis=0, inplace=True)

"""Melakukan pengecekan jumlah data null pada semua kolom dalam tabel `orders_df` untuk memastikan proses dropping data null berhasil."""

orders_df.isna().sum()

"""#### **4. Membersihkan tabel `products_df`**

Melakukan dropping pada semua kolom yang memiliki `missing value`. Alasan tersebut didasari karena jumlah data dalam kolom yang miliki missing value relatif sedikit jika dibandingkan dengan kesuluruhan data.
"""

products_df.dropna(axis=0, inplace=True)

"""Melakukan pengecekan jumlah data null pada semua kolom dalam tabel `products_df` untuk memastikan proses dropping data null berhasil."""

products_df.isna().sum()

"""#### **5. Membersihkan tabel `geolocation_df`**

Melakukan `proses reduksi` untuk menghasilkan DataFrame yang lebih ringkas yang nantinya akan digunakan dalam analisis data. Proses reduksi tersebut dilakukan pada data yang dianggap redundan atau tidak memberikan nilai tambah signifikan untuk analisis yang sedang dilakukan.
"""

geolocation_df.info()

# Step 1: Identify zip codes with more than one state

"""
- Pada langkah pertama, dilakukan pengelompokan berdasarkan kolom 'geolocation_zip_code_prefix'
untuk mengidentifikasi berapa banyak negara bagian yang terkait dengan setiap kode pos.
- Kode ini kemudian menghasilkan DataFrame baru, yaitu multi_state_zipcodes, yang hanya berisi baris
yang unik untuk setiap kombinasi kode pos dan negara bagian.
"""

multi_state_zipcodes = (
    geolocation_df.groupby('geolocation_zip_code_prefix')['geolocation_state']
    .nunique().reset_index(name='state_count')
)

# Count zip codes with more than one state
num_multi_state_zipcodes = multi_state_zipcodes[multi_state_zipcodes['state_count'] >= 2].shape[0]

# Step 2: Find the most frequent state for each zip code

"""
- Selanjutnya, dilakukan pengelompokan berdasarkan kolom 'geolocation_zip_code_prefix' dan 'geolocation_state'
untuk menghitung frekuensi munculnya setiap kombinasi kode pos dan negara bagian.
- Hasilnya diambil hanya kombinasi yang paling sering muncul untuk setiap kode pos dengan menghapus duplikat berdasarkan 'geolocation_zip_code_prefix'.
- DataFrame baru, most_frequent_state, dibuat yang hanya berisi kombinasi yang paling sering muncul.
"""

most_frequent_state = (
    geolocation_df.groupby(['geolocation_zip_code_prefix', 'geolocation_state'])
    .size().reset_index(name='state_count')
    .drop_duplicates(subset='geolocation_zip_code_prefix')
    .drop('state_count', axis=1)
)

# Step 3: Create a DataFrame with median latitude and longitude

"""
- Dilakukan pengelompokan berdasarkan kolom 'geolocation_zip_code_prefix', 'geolocation_city', dan 'geolocation_state'
untuk menghitung nilai median dari 'geolocation_lat' dan 'geolocation_lng'.
- DataFrame hasilnya, median_geolocation, kemudian digabungkan dengan most_frequent_state
berdasarkan kolom 'geolocation_zip_code_prefix' dan 'geolocation_state'.
- DataFrame ini berisi nilai median letak geografis untuk setiap kode pos, kota, dan negara bagian,
di mana negara bagiannya adalah yang paling sering muncul untuk setiap kode pos
"""

median_geolocation = (
    geolocation_df.groupby(['geolocation_zip_code_prefix', 'geolocation_city', 'geolocation_state'])
    [['geolocation_lat', 'geolocation_lng']]
    .median().reset_index()
    .merge(most_frequent_state, on=['geolocation_zip_code_prefix', 'geolocation_state'], how='inner')
)

median_geolocation

median_geolocation.info()

"""DataFrame yang dihasilkan pada proses pembersihan ini hanya berisi informasi yang `lebih ringkas dan representatif` tentang letak geografis untuk setiap kode pos dan negara bagian yang paling sering muncul. Reduksi besar dalam jumlah baris ini tidak selalu menunjukkan kehilangan informasi yang signifikan. Sebaliknya, hal tersebut mencerminkan proses agregasi dan pembersihan data yang dilakukan untuk mempertahankan informasi kunci dalam data.

Jika diperhatikan, jumlah baris pada tabel geolocation_df sebelum dan sesudah dilakukan proses reduksi memiliki perbedaan. Jumlah baris `sebelum` dilakukan reduksi yaitu sebanyak `1000163 baris`, dan jumlah baris `sesudah` dilakukan reduksi sebanyak `27901 baris`.

### **Exploratory Data Analysis (EDA)**

### Explore `customers_df`
"""

customers_df.sample(5)

customers_df.describe(include="all")

customers_df.customer_unique_id.is_unique

customers_df.customer_unique_id.duplicated

customers_df.groupby(by="customer_city").customer_unique_id.nunique().sort_values(ascending=False)

customers_df.groupby(by="customer_state").customer_unique_id.nunique().sort_values(ascending = False)

"""### Explore `sellers_df`"""

sellers_df.sample(5)

sellers_df.describe(include="all")

sellers_df.groupby(by="seller_city").seller_id.nunique().sort_values(ascending=False)

sellers_df.groupby(by="seller_state").seller_id.nunique().sort_values(ascending=False)

"""### Explore `geolocation_df`"""

geolocation_df

state_city_counts = geolocation_df.groupby(['geolocation_state', 'geolocation_city']).size().reset_index(name='count')
state_city_counts = state_city_counts.sort_values(by='count', ascending=False)

plt.figure(figsize=(10, 7))
barplot = sns.barplot(x='geolocation_state', y='count', hue='geolocation_city', data=state_city_counts.head(10), palette='pastel')

# Menambahkan nilai pada setiap bar
for p in barplot.patches:
    barplot.annotate(f'{p.get_height():,.0f}', (p.get_x() + p.get_width() / 2., p.get_height()),
                     ha='center', va='center', xytext=(0, 9), textcoords='offset points', fontsize=6)

plt.title('Top 10 Cities in Each State')
plt.xlabel('State')
plt.ylabel('Count')
plt.legend(title='City', loc='upper right')
plt.show()

"""### Explore `customers_df` dan `geolocation_df`

Proses explore pada kedua tabel ini akan dilakukan dengan menggabungkan kedua tabel yaitu `customers_df` serta hasil dari pembersihan tabel `geolocation_df`, yaitu `median_geolocation` menggunakan kolom `customer_zip_code_prefix` dan `geolocation_zip_code_prefix`.
"""

# Melakukan merge terhadap customers_df dan median_geolocation
customers_with_median_geolocation = pd.merge(
    left=customers_df,
    right=median_geolocation,
    how='inner',
    left_on='customer_zip_code_prefix',
    right_on='geolocation_zip_code_prefix',
)

customers_with_median_geolocation.head()

customers_with_median_geolocation.info()

customers_with_median_geolocation.describe()

"""Selanjutnya, data dari proses penggabungan kedua tabel tersebut akan di simpan dalam format csv, agar nantinya dapat digunakan pada visualisasi geolocation dalam dashboard."""

# customers_with_median_geolocation.to_csv("geolocation.csv", index=False)

"""### Explore `orders_df`"""

# Buat kolom baru untuk menghitung selisih waktu pembelian barang dan pada saat barang tersebut sampai pada customer
delivery_time = orders_df['order_delivered_customer_date'] - orders_df['order_purchase_timestamp']
orders_df["delivery_time (days)"] = round(delivery_time / pd.to_timedelta(1, unit='D'))

orders_df.sample(5)

status_pesanan = orders_df[orders_df['order_status'] == 'canceled']

status_pesanan

orders_df.describe(include="all")

orders_df['delivery_time (days)'].hist()

customer_id_in_orders_df =  orders_df.customer_id.tolist()
customers_df["status"] = customers_df["customer_id"].apply(lambda x: "Active" if x in customer_id_in_orders_df else "Non Active")
customers_df.sample(5)

customers_df.groupby(by="status").customer_id.count()

orders_df.groupby(by="order_status").order_id.count()

"""**FYI :** Jika diperhatikan, jumlah dari customer active atau customer yang pernah melakukan riwayat pemesanan sejumlah `96461`. Hal tersebut cocok dengan jumlah order status yaitu pesanan dibatalkan sebanyak `6` dan pesanan terkirim sebanyak `96455`, yang kalau dijumlahkan secara kesuluruhan maka sama dengan jumlah customer active, yaitu `96461`.

### Explore `orders_df` & `customers_df`
"""

# Melakukan merge terhadap customers dan orders
customers_orders_df = pd.merge(
    left=customers_df,
    right=orders_df,
    how="right",
    left_on="customer_id",
    right_on="customer_id"
)

customers_orders_df.head()

customers_orders_df.groupby(by="customer_city").order_id.nunique().sort_values(ascending=False).head(10)

customers_orders_df.groupby(by="customer_state").order_id.nunique().sort_values(ascending=False).head(10)

"""### Explore `products_df` & `product_category_name_df`"""

products_df.sample(5)

product_category_name_df.sample(5)

"""Disini kita bisa menggabungkan kedua tabel tersebut berdasarkan kolom `product_category_name`"""

# Melakukan merge terhadap product_category_name_df dan products_df
products_product_category_name_df = pd.merge(
    left=product_category_name_df,
    right=products_df,
    how="right",
    left_on="product_category_name",
    right_on="product_category_name"
)

products_product_category_name_df.head()

"""### Explore `sellers_df`, `order_items_df`, & `order_payments_df`"""

sellers_df.sample(5)

order_items_df.sample(5)

"""Disini kita bisa menggabungkan kedua tabel tersebut berdasarkan kolom `seller_id`"""

# Melakukan merge terhadap sellers_df dan order_items_df
sellers_order_items_df = pd.merge(
    left=sellers_df,
    right=order_items_df,
    how="right",
    left_on="seller_id",
    right_on="seller_id"
)

sellers_order_items_df.head()

order_payments_df.sample(5)

"""Selanjutnya, kita akan mencoba menggabungkan hasil dari penggabungan tabel `sellers` dan `order_items` sebelumnya dengan tabel `order_payments` berdasarkan kolom `order_id`"""

# Melakukan merge terhadap sellers_order_items dan order_payments
sellers_order_payments_df = pd.merge(
    left=sellers_order_items_df,
    right=order_payments_df,
    how="right",
    left_on="order_id",
    right_on="order_id"
)

sellers_order_payments_df.head()

sellers_order_payments_df.info()

# Lokasi penjual terbanyak
sellers_order_payments_df.groupby(['seller_city', 'seller_state']).size().idxmax()

# Total pembayaran per metode pembayaran
sellers_order_payments_df.groupby('payment_type')['payment_value'].sum().sort_values(ascending = False)

#Rata-rata total pembayaran untuk setiap metode pembayaran.
sellers_order_payments_df.groupby('payment_type')['payment_value'].mean().sort_values(ascending = False)

#Korelasi antara harga produk dan biaya pengiriman.
sellers_order_payments_df['price'].corr(sellers_order_payments_df['freight_value'])

"""Nilai `0.4158` yang positif menunjukkan bahwa, secara umum, ketika harga produk naik, biaya pengiriman juga cenderung naik, dan sebaliknya. Korelasi sebesar `0.4158` termasuk dalam kategori hubungan yang moderat, yang menandakan bahwa `korelasinya tidak terlalu kuat`, tetapi `ada hubungan yang cukup signifikan` antara `harga produk` dan `biaya pengiriman`.

### Explore `products_product_category_name_df`, & `sellers_order_payments`
"""

products_product_category_name_df.sample(5)

sellers_order_payments_df.sample(5)

"""Dalam explore data kali ini, kita akan mencoba menggabungkan tabel `products_product_category_name_df` dan `sellers_order_payments_df` berdasarkan kolom `product_id`"""

# Melakukan merge terhadap products_product_category_name_df dan sellers_order_payments_df
products_payments_df = pd.merge(
    left=products_product_category_name_df,
    right=sellers_order_payments_df,
    how="right",
    left_on="product_id",
    right_on="product_id"
)

products_payments_df.head()

products_payments_df.info()

products_payments_df.groupby(by=['payment_type', 'product_category_name_english']).agg({
    "order_item_id": "sum",
    "price": "sum",
})

"""### Explore `products_payments_df`, & `order_reviews_df`

Setelah melakukan penggabungan dari pada tabel `products_product_category_name_df` dan `sellers_order_payments`, selanjutnya kita akan melakukan penggabungan dari hasil kedua tabel tersebut dengan `order_reviews_df` untuk mengetahui reviews dari costumers terkait produk yang dibeli berdasarkan kolom `order_id`
"""

order_reviews_df.sample(5)

# Melakukan merge terhadap products_payments_df dan order_reviews_df
products_reviews_merged_df = pd.merge(
    left=products_payments_df,
    right=order_reviews_df,
    how="right",
    left_on="order_id",
    right_on="order_id"
)

products_reviews_merged_df

products_reviews_merged_df.info()

products_reviews_merged_df[['price', 'review_score']].corr()

"""Korelasi antara `price` dan `review_score` sangat dekat dengan 0, yaitu `-0.004515`. Ini menunjukkan bahwa `tidak ada korelasi yang signifikan` antara `harga (price)` produk dan `skor ulasan (review_score)`. Dengan kata lain, perubahan dalam `harga produk tidak memiliki dampak yang konsisten pada skor ulasan pelanggan, dan sebaliknya`.

Selanjutnya, saya mencoba untuk menganalisa jumlah persentase pada masing-masing value dari review_score.
"""

review_score_distribution = (products_reviews_merged_df['review_score'].value_counts(normalize=True) * 100).round(0).astype(int)
review_score_distribution.astype(str) + '%'

"""### Explore `All Data`

Explore terakhir yang akan dilakukan adalah menggabungkan hasil merge tabel `customers` dan `orders` yaitu `customers_orders_df`, dan hasil merge tabel sebelumnya yaitu `products_reviews_merged_df`. Hasil dari merge tersebut akan ditampung dalam data frame baru yaitu `all_df`
"""

customers_orders_df.sample(5)

products_reviews_merged_df.sample(5)

# Melakukan merge terhadap products_reviews_merged_df dan customers_orders_df
all_df = pd.merge(
    left=products_reviews_merged_df,
    right=customers_orders_df,
    how="right",
    left_on="order_id",
    right_on="order_id"
)

all_df.sample(5)

all_df.info()

"""Setelah menggabungkan semua kolom dari proses explore yang sudah dilakukan diatas, maka didapatkan sebanyak `42 kolom`. Semua kolom tersebut nantinya akan saya simpan dalam file `main_data.csv` untuk digunakan dalam proses perancangan `dashboard`."""

avg_payment_info = all_df.groupby('payment_type').agg({'price': 'mean', 'freight_value': 'mean'})
avg_payment_info

all_df.groupby('review_score').agg({'price': 'sum', 'freight_value': 'sum'})

## all_df.to_csv("main_data.csv", index=False)

"""### **Visualization & Explanatory Analysis**

### **Bagaimana demografi pelanggan yang kita miliki?**

#### **Berdasarkan Negara Bagian**
"""

bystate_df = all_df.groupby(by="customer_state").customer_id.nunique().reset_index()
bystate_df.rename(columns={
    "customer_id": "customer_count"
}, inplace=True)

# Buat dictionary pemetaan
state_mapping = {
    'SP': 'SP (SÃ£o Paulo)',
    'RJ': 'RJ (Rio de Janeiro)',
    'MG': 'MG (Minas Gerais)',
    'RS': 'RS (Rio Grande do Sul)',
    'PR': 'PR (ParanÃ¡)'
}

colors_ = ["#38BDF8", "#D3D3D3", "#D3D3D3", "#D3D3D3", "#D3D3D3"]

# Terapkan pemetaan pada DataFrame
bystate_df['customer_state'] = bystate_df['customer_state'].replace(state_mapping)

# Plotting
plt.figure(figsize=(10, 6))
ax = sns.barplot(
    y="customer_count",
    x="customer_state",
    data=bystate_df.sort_values(by="customer_count", ascending=False).head(5),
    palette=colors_,
    hue="customer_state",
    legend=False
)
plt.title("Number of Customer by State", loc="center", fontsize=15)
plt.ylabel(None)
plt.xlabel(None)
plt.tick_params(axis='x', labelsize=12, rotation=35)

# Menambahkan label nilai pada setiap bar
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points')

plt.show()

"""#### **Berdasarkan Letak Geografis Pelanggan**"""

brazil = mpimg.imread(urllib.request.urlopen('https://i.pinimg.com/originals/3a/0c/e1/3a0ce18b3c842748c255bc0aa445ad41.jpg'), 'jpg')

# Create a scatter plot
ax = customers_with_median_geolocation.drop_duplicates(subset='customer_unique_id').plot(
    kind="scatter", x="geolocation_lng", y="geolocation_lat", figsize=(10, 10), alpha=0.3, s=0.3, c='maroon'
)

# Turn off the axis
plt.axis('off')

# Overlay the map image on the scatter plot
plt.imshow(brazil, extent=[-73.98283055, -33.8, -33.75116944, 5.4])

# Show the plot
plt.show()

"""Berdasarkan visualisasi yang telah disajikan, terlihat bahwa jumlah pelanggan lebih tinggi di wilayah `tenggara` dan `selatan`. Selain itu, data juga mengindikasikan bahwa kota-kota yang menjadi pusat pemerintahan, seperti SÃ£o Paulo, Rio de Janeiro, dan lain-lain, memiliki jumlah pelanggan yang lebih besar.

#### **Berdasarkan Tipe Pembayaran**
"""

bypayment_df = all_df.groupby(by="payment_type").customer_id.nunique().reset_index()
bypayment_df.rename(columns={
    "customer_id": "customer_count"
}, inplace=True)

plt.figure(figsize=(10, 5))

colors_ = ["#38BDF8", "#D3D3D3", "#D3D3D3", "#D3D3D3"]
ax = sns.barplot(
    y="customer_count",
    x="payment_type",
    data=bypayment_df.sort_values(by="customer_count", ascending=False),
    palette=colors_,
    hue="payment_type",
    legend=False
)
plt.title("Number of Customer by Payment Type", loc="center", fontsize=15)
plt.ylabel(None)
plt.xlabel(None)
plt.tick_params(axis='x', labelsize=12)

# Menambahkan label nilai pada setiap bar
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points')

plt.show()

"""#### **Berdasarkan Review Score**"""

byrating_df = all_df.groupby(by="review_score").customer_id.nunique().reset_index()
byrating_df.rename(columns={
    "customer_id": "customer_count"
}, inplace=True)

byrating_df['review_score'] = pd.Categorical(byrating_df['review_score'], [5, 4, 3, 2, 1])

colors_ = ["#38BDF8", "#D3D3D3", "#D3D3D3", "#D3D3D3",  "#D3D3D3"]

plt.figure(figsize=(10, 6))

ax = sns.barplot(
    y="customer_count",
    x="review_score",
    data=byrating_df.sort_values(by="customer_count", ascending=False),
    palette=colors_,
    hue="review_score",
    legend=False
)
plt.title("Number of Customer by Review Score", loc="center", fontsize=15)
plt.ylabel(None)
plt.xlabel(None)
plt.tick_params(axis='x', labelsize=12)

# Menambahkan label nilai pada setiap bar
for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='center', xytext=(0, 5), textcoords='offset points')

plt.show()

"""### **Produk apa yang paling banyak dan paling sedikit terjual?**"""

product_sales = all_df.groupby("product_category_name_english").order_item_id.sum().sort_values(ascending=False).reset_index()

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(24, 6))

colors = ["#38BDF8", "#D3D3D3", "#D3D3D3", "#D3D3D3", "#D3D3D3"]

sns.barplot(x="product_category_name_english", y="order_item_id", data=product_sales.head(5), palette=colors, hue="product_category_name_english", ax=ax[0], legend=False)
ax[0].set_ylabel(None)
ax[0].set_xlabel(None)
ax[0].set_title("Best Performing Product", loc="center", fontsize=18)
ax[0].tick_params(axis ='y', labelsize=15)
ax[0].tick_params(axis='x', labelsize=13, rotation=45)

# Menambahkan nilai di sumbu x untuk ax[0]
for p in ax[0].patches:
    ax[0].annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')


sns.barplot(x="product_category_name_english", y="order_item_id", data=product_sales.tail(5).sort_values(by="order_item_id", ascending=True), palette=colors, hue="product_category_name_english", ax=ax[1], legend=False)
ax[1].set_ylabel(None)
ax[1].set_xlabel(None)
ax[1].invert_xaxis()
ax[1].yaxis.set_label_position("right")
ax[1].yaxis.tick_right()
ax[1].set_title("Worst Performing Product", loc="center", fontsize=18)
ax[1].tick_params(axis='y', labelsize=15)
ax[1].tick_params(axis='x', labelsize=13, rotation=45)

for p in ax[1].patches:
    ax[1].annotate(f'{p.get_height():.0f}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=12, color='black', xytext=(0, 5), textcoords='offset points')

plt.suptitle("Best and Worst Performing Product by Number of Sales", fontsize=20)
plt.show()

"""### **Bagaimana performa penjualan dan revenue perusahaan dalam setiap bulan (2016-09 sampai 2018-08) ?**"""

all_df['order_month'] = all_df['order_purchase_timestamp'].dt.to_period('M')
monthly_orders = all_df.groupby('order_month')['order_id'].nunique()
monthly_revenue = all_df.groupby('order_month')['price'].sum()

# Menggabungkan hasil pengelompokan menjadi satu DataFrame
result_df = pd.DataFrame({
    'order_month': monthly_revenue.index,
    'monthly_orders': monthly_orders.values,
    'monthly_revenue': monthly_revenue.values
})

# Menampilkan hasil
result_df

monthly_revenue.index = monthly_revenue.index.astype(str)
monthly_orders.index = monthly_orders.index.astype(str)

# Mengatur lebar dan tinggi gambar
fig, ax1 = plt.subplots(figsize=(10, 7))

color = 'tab:red'
ax1.set_xlabel('Month')
ax1.set_ylabel('Total Revenue', color=color)
ax1.plot(monthly_revenue.index, monthly_revenue, color=color, label='Total Revenue')
ax1.tick_params(axis='y', labelcolor=color)

ax2 = ax1.twinx()
color = 'tab:blue'
ax2.set_ylabel('Number of Orders', color=color)
ax2.plot(monthly_orders.index, monthly_orders, color=color, label='Number of Orders')
ax2.tick_params(axis='y', labelcolor=color)

# Mengatur posisi tick dan label pada sumbu x
ax1.set_xticks(ax1.get_xticks())
ax1.set_xticklabels(monthly_revenue.index, rotation=45, ha='right')

# Menambahkan keterangan (legend) di sebelah kiri atas
lines, labels = ax1.get_legend_handles_labels()
lines2, labels2 = ax2.get_legend_handles_labels()
ax2.legend(lines + lines2, labels + labels2, loc='upper left', bbox_to_anchor=(0, 1))

fig.tight_layout()
plt.title('Number of Orders and Total Revenue per Month (2016-2018)')
plt.show()

"""### **RFM Analysis**"""

rfm_df = all_df.groupby(by="customer_id", as_index=False).agg({
    "order_purchase_timestamp": "max",
    "order_id": "nunique",
    "price": "sum"
})
rfm_df.columns = ["customer_id", "max_order_timestamp", "frequency", "monetary"]
rfm_df.head()

rfm_df["max_order_timestamp"] = rfm_df["max_order_timestamp"].dt.date
recent_date = orders_df["order_purchase_timestamp"].dt.date.max()
rfm_df["recency"] = rfm_df["max_order_timestamp"].apply(lambda x: (recent_date - x).days)
rfm_df.head()

rfm_df.drop("max_order_timestamp", axis=1, inplace=True)
rfm_df.head()

rfm_df.describe()

rfm_df.sort_values(by="frequency", ascending=False).head(5)

rfm_df.sort_values(by="monetary", ascending=False).head(5)

fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(12, 12))

colors = ["#72BCD4", "#72BCD4", "#72BCD4", "#72BCD4", "#72BCD4"]

# Grafik pertama (By Frequency)
freq_plot = sns.barplot(
    y="customer_id",
    x="frequency",
    data=rfm_df.sort_values(by="frequency", ascending=False).head(5),
    palette=colors,
    hue="frequency",
    legend=False,
    ax=ax[0]
)
ax[0].set_ylabel("Customer ID")
ax[0].set_xlabel(None)
ax[0].set_title("By Frequency", loc="center", fontsize=18)
ax[0].tick_params(axis='x', labelsize=15)

# Menambahkan nilai pada grafik pertama
for p in freq_plot.patches:
    ax[0].annotate(f'{p.get_width():,.0f}', (p.get_width(), p.get_y() + p.get_height() / 2),
                ha='center', va='center', xytext=(5, 0), textcoords='offset points', fontsize=12)

# Grafik kedua (By Monetary)
monetary_plot = sns.barplot(
    y="customer_id",
    x="monetary",
    data=rfm_df.sort_values(by="monetary", ascending=False).head(5),
    palette=colors,
    hue="monetary",
    legend=False,
    ax=ax[1]
)
ax[1].set_ylabel("Customer ID")
ax[1].set_xlabel(None)
ax[1].set_title("By Monetary", loc="center", fontsize=18)
ax[1].tick_params(axis='x', labelsize=15)

# Menambahkan nilai pada grafik kedua
for p in monetary_plot.patches:
    ax[1].annotate(f'R${p.get_width():,.2f}', (p.get_width(), p.get_y() + p.get_height() / 2),
                ha='center', va='center', xytext=(5, 0), textcoords='offset points', fontsize=12)

plt.suptitle("Best Customer Based on RFM Parameters (customer_id)", fontsize=20)
plt.show()

"""### **Mengurutkan customer berdasarkan recency, frequency, & monetary score**"""

rfm_df['r_rank'] = rfm_df['recency'].rank(ascending=False)
rfm_df['f_rank'] = rfm_df['frequency'].rank(ascending=True)
rfm_df['m_rank'] = rfm_df['monetary'].rank(ascending=True)

rfm_df.head()

# normalizing the rank of the customers
rfm_df['r_rank_norm'] = (rfm_df['r_rank']/rfm_df['r_rank'].max())*100
rfm_df['f_rank_norm'] = (rfm_df['f_rank']/rfm_df['f_rank'].max())*100
rfm_df['m_rank_norm'] = (rfm_df['m_rank']/rfm_df['m_rank'].max())*100

rfm_df.drop(columns=['r_rank', 'f_rank', 'm_rank'], inplace=True)

rfm_df.head()

rfm_df['RFM_score'] = 0.15*rfm_df['r_rank_norm']+0.28 * \
    rfm_df['f_rank_norm']+0.57*rfm_df['m_rank_norm']
rfm_df['RFM_score'] *= 0.05
rfm_df = rfm_df.round(2)
rfm_df[['customer_id', 'RFM_score']].head(7)

"""### **Segmentasi customer berdasarkan RFM_score**"""

rfm_df["customer_segment"] = np.where(
    rfm_df['RFM_score'] > 4.5, "Top customers", (np.where(
        rfm_df['RFM_score'] > 4, "High value customer",(np.where(
            rfm_df['RFM_score'] > 3, "Medium value customer", np.where(
                rfm_df['RFM_score'] > 1.6, 'Low value customers', 'lost customers'))))))

rfm_df[['customer_id', 'RFM_score', 'customer_segment']].head(20)

customer_segment_df = rfm_df.groupby(by="customer_segment", as_index=False).customer_id.nunique()
customer_segment_df

customer_segment_df['customer_segment'] = pd.Categorical(customer_segment_df['customer_segment'], [
    "lost customers", "Low value customers", "Medium value customer",
    "High value customer", "Top customers"
])

plt.figure(figsize=(10, 5))
colors_ = ["#72BCD4", "#72BCD4", "#D3D3D3", "#D3D3D3", "#D3D3D3"]

# Membuat plot bar
ax = sns.barplot(
    x="customer_id",
    y="customer_segment",
    data=customer_segment_df.sort_values(by="customer_segment", ascending=False),
    palette=colors_,
    hue="customer_segment",
    legend=False
)

plt.title("Number of Customer for Each Segment", loc="center", fontsize=15)
plt.ylabel(None)
plt.xlabel(None)
plt.tick_params(axis='y', labelsize=12)

# Menambahkan nilai pada setiap batang bar
for p in ax.patches:
    ax.annotate(f'{p.get_width()}',
                (p.get_width(), p.get_y() + p.get_height() / 2),
                ha='left', va='center', xytext=(5, 0), textcoords='offset points', fontsize=10)

plt.show()

# Mengetahui jumlah uang yang dihabiskan oleh semua pelanggan
total_monetary = rfm_df["monetary"].sum()

# Menampilkan hasil
print("Total Monetary Spent by Customers:", total_monetary)

segmented_monetary = rfm_df.groupby("customer_segment")["monetary"].sum().sort_values(ascending=False)

# Membuat plot bar
plt.figure(figsize=(10, 5))
colors_ = ["#72BCD4", "#D3D3D3", "#D3D3D3", "#D3D3D3", "#D3D3D3"]

# Membuat plot bar
ax = sns.barplot(
    x=segmented_monetary.values,
    y=segmented_monetary.index,
    palette=colors_,
    hue=segmented_monetary.index,
    legend=False
)

plt.title("Total Monetary for Each Customer Segment", loc="center", fontsize=15)
plt.xlabel("Total Monetary (R$)", fontsize=12)
plt.ylabel("Customer Segment", fontsize=12)
plt.tick_params(axis='x', labelsize=12)

# Menambahkan nilai mata uang R$ di awal setiap nilai
for p in ax.patches:
    ax.annotate(f'R${p.get_width():,.2f}',
                (p.get_width(), p.get_y() + p.get_height() / 2),
                ha='left', va='center', xytext=(5, 0), textcoords='offset points', fontsize=10)

plt.show()

"""# **Conclusion**.

### **Hasil dari Visualization & Explanatory Analysis**.

**1. Bagaimana demografi pelanggan yang kita miliki ?**
   - Berdasarkan Negara Bagian
     > Dari proses analisis yang dijabarkan dalam bentuk visualisasi sebelumnya, dapat diketahui bahwa 5 negara bagian dengan pelanggan terbesar yaitu, `SP (SÃ£o Paulo)` dengan jumlah 40.489 pelanggan, disusul `RJ (Rio de Janeiro)` dengan 12,351 pelanggan, selanjutnya `MG (Minas Gerais)` dengan 11.352 pelanggan, kemudian `RS (Rio Grande do Sul)` dengan 5.342 pelanggan, dan `PR (ParanÃ¡)` dengan 4.923 pelanggan.
     
   - Berdasarkan Letak Geografis Pelanggan
     > Berdasarkan visualisasi yang telah disajikan, terlihat bahwa jumlah pelanggan lebih tinggi di wilayah `tenggara` dan `selatan`. Selain itu, data juga mengindikasikan bahwa kota-kota yang menjadi pusat pemerintahan, seperti SÃ£o Paulo, Rio de Janeiro, dan lain-lain, memiliki jumlah pelanggan yang lebih besar.

   - Berdasarkan Tipe Pembayaran
     > Bersumber pada proses analisis yang sudah dilakukan pada tipe pembayaran yang digunakan oleh pelanggan, diketahui bahwa tipe pembayaran yang `paling banyak digunakan oleh pelanggan adalah credit_card`. Hal tersebut dibuktikan dengan jumlah pelanggan yang menggunakan tipe pembayaran tersebut, yaitu sebanyak `73.806 pelanggan`, disusul oleh tipe pembayaran `boleto` dengan jumlah pelanggan sebanyak `19.048`, dilanjutkan `voucher` dengan `3.648 pelanggan`, dan `debit_card` dengan jumlah `1.477 pelanggan`.
     
   - Berdasarkan Review Score
     > Jika dilihat dari barplot yang ditampilkan dalam tahap Visualization & Explanatory Analysis, diketahui bahwa `review score dengan angka 5 memiliki jumlah yang lebih banyak` dibandingkan dengan angka lainnya, yaitu sebesar `43.62%` atau `56.802 pelanggan`. Hal tersebut membuktikan bahwa `hasil dari penjualan direspon positif oleh pelanggan`.

**2. Produk apa yang paling banyak dan paling sedikit terjual?**
   > Dari tahap analisis yang sudah dilakukan, terlihat bahwa produk dengan `penjualan paling laris yaitu bed_bath_table` dengan jumlah 14.429 penjualan. Sedangkan, produk dengan `penjualan paling sedikit yaitu security_and_services` dengan jumlah 2 penjualan saja.

**3. Bagaimana performa penjualan dan revenue perusahaan dalam setiap bulan (2016-09 sampai 2018-08) ?**
   > Secara keseluruhan terlihat bahwa performa penjualan dan pendapatan perusahaan menunjukkan `tren positif` dimulai dari angka yang rendah, yaitu di September 2016 (total pendapatan R\$0 dan 1 order) hingga mencapai `puncaknya di November 2017` (total pendapatan R\$1,022,461 dan 7,288 order). Pertumbuhan yang sangat pesat ini menandakan `perkembangan bisnis yang sangat baik` dari 2016 hingga 2017. Walaupun di tahun 2018 terjadi `perlambatan` dengan adanya penurunan pendapatan dan jumlah order di beberapa bulan dibandingkan tahun 2017, namun secara keseluruhan, performa penjualan dan pendapatan masih meningkat dari tahun ke tahun. Perusahaan perlu menerapkan `strategi yang tepat` seperti inovasi produk atau perluasan pasar untuk mempertahankan dan meningkatkan kinerja penjualan serta pendapatannya agar dapat terus tumbuh secara berkelanjutan.

### **Hasil dari RFM Analysis**.

RFM adalah singkatan dari Recency, Frequency, dan Monetary, yang digunakan dalam analisis pelanggan untuk memahami dan membagi pelanggan berdasarkan tiga dimensi utama:

1. **Recency (R):** Menunjukkan parameter yang digunakan untuk melihat kapan terakhir seorang pelanggan melakukan transaksi.

2. **Frequency (F):** Merupakan parameter yang digunakan untuk mengidentifikasi seberapa sering seorang pelanggan melakukan transaksi.

3. **Monetary (M):** Parameter terakhir ini digunakan untuk mengidentifikasi seberapa besar revenue yang berasal dari pelanggan tersebut.

Dari data yang diberikan, terdapat total uang yang dihabiskan oleh seluruh pelanggan sebesar `R$ 13,754,684.09`. Selanjutnya, jumlah uang yang dihabiskan oleh pelanggan pada masing-masing segmen adalah sebagai berikut:

- **High Value Customer:** Pelanggan dengan nilai monetary tertinggi, yakni `R$ 4,871,354.85`. Ini menunjukkan bahwa pelanggan dalam segmen ini memiliki kontribusi signifikan terhadap total pendapatan.

- **Low Value Customers:** Pelanggan dalam segmen ini menghabiskan total uang sebesar `R$ 1,736,014.73`. Meskipun frekuensi pembelian mereka mungkin tidak sebanyak segmen lain, total uang yang dihabiskan tetap cukup signifikan.

- **Medium Value Customer:** Pelanggan dalam segmen ini menghabiskan total uang sebesar `R$ 3,959,947.10`. Mereka berada di tengah-tengah antara pelanggan dengan nilai monetary tertinggi dan terendah.

- **Top Customers:** Pelanggan dalam segmen ini menghabiskan total uang sebesar `R$ 3,174,911.11`. Meskipun tidak sebesar High Value Customer, namun tetap memberikan kontribusi yang signifikan.

- **Lost Customers:** Pelanggan dalam segmen ini menghabiskan total uang sebesar `R$ 12,456.30`. Meskipun jumlahnya lebih kecil dibandingkan segmen lain, tetapi tetap memberikan dampak pada total pendapatan.

Jadi, dari hasil analisis RFM, dapat dilihat bagaimana kontribusi setiap segmen pelanggan terhadap total uang yang dihabiskan dan bagaimana mereka berperilaku dalam hal recency, frequency, dan monetary.
"""
# -*- coding: utf-8 -*-
"""Project Analisis Data-Abdul Rahman Wahid.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18X5CBWVQXdHCaeE2u_Q4vXa8TOYAvVj4

Belajar Analisis Data dengan Python

**Proyek Analisis Data: E-Commerce Public Dataset**

*   Name      : Abdul Rahman Wahid
*   Email     : wahidabdul2801@gmail.com
*   Username  : abdulrwahid


---


This is my submission on IDCAMP 2023 Data Scientist Path.

For all viewers:
Don't plagiarize my code, I will hide and delete my github account☹

Dataset:
https://drive.google.com/file/d/1MsAjPM7oKtVfJL_wRp1qmCajtSG1mdcK/view



---

# **BUSINESS QUESTION**



1. What are the best-selling products and what are not?
2. How much budget has the customer spent in the last few months?
3. How is the sales performance on the E-Commerce platform?
4. What is the level of customer satisfaction with our services?
5. What are the demographic profiles of customers, and are there any differences in purchasing preferences among them?
6. Based on geographical location, where are the majority of customers located?

# **IMPORT LIBRARY**
"""

#Import required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import urllib
import matplotlib.image as mpimg
from scipy import stats

"""# **DATA WRANGLING**

# **Gathering Data**

# Customer Dataset
"""

customers_df = pd.read_csv('/content/customers_dataset.csv')
customers_df.head()

"""# Order Items Dataset"""

order_items = pd.read_csv('/content/order_items_dataset.csv')
order_items.head()

"""# Order Pay Dataset"""

order_pay = pd.read_csv('/content/order_payments_dataset.csv')
order_pay.head()

"""# Order Review Dataset"""

order_rev = pd.read_csv('/content/order_reviews_dataset.csv')
order_rev.head()

"""# Product Category Dataset"""

product_cat = pd.read_csv('/content/product_category_name_translation.csv')
product_cat.head()

"""# Order Dataset"""

orders_df = pd.read_csv('/content/orders_dataset.csv')
orders_df.head()

"""# Products Dataset"""

products_df = pd.read_csv('/content/products_dataset.csv')
products_df.head()

"""# Seller Dataset"""

sellers_df = pd.read_csv('/content/sellers_dataset.csv')
sellers_df.head()

"""# Geolocation Dataset"""

geo_df = pd.read_csv('/content/geolocation_dataset.csv')
geo_df.head()

"""# **Data Assesing**

Check for Data Types in Every Dataset
"""

print('\n', customers_df.info())
print('\n', geo_df.info())
print('\n', order_items.info())
print('\n', order_pay.info())
print('\n', order_rev.info())
print('\n', orders_df.info())
print('\n', product_cat.info())
print('\n', products_df.info())
print('\n', sellers_df.info())

# Directory containing the CSV files
data_dir = '/content/sample_data/'

# Access and Check every Dataset
orders = pd.read_csv(data_dir + 'orders_dataset.csv')
items = pd.read_csv(data_dir + 'order_items_dataset.csv')
products = pd.read_csv(data_dir + 'products_dataset.csv')
payments = pd.read_csv(data_dir + 'order_payments_dataset.csv')
reviews = pd.read_csv(data_dir + 'order_reviews_dataset.csv')
customers = pd.read_csv(data_dir + 'customers_dataset.csv')
sellers = pd.read_csv(data_dir + 'sellers_dataset.csv')
category = pd.read_csv(data_dir + 'product_category_name_translation.csv')
geolocation = pd.read_csv(data_dir + 'geolocation_dataset.csv')

data = {'orders': orders,
        'items': items,
        'products': products,
        'payments': payments,
        'reviews': reviews,
        'customers': customers,
        'sellers': sellers,
        'category': category,
        'geo': geolocation}

for df_name, df in data.items():
  print('\n', data[df_name].info())

"""Check Null Values"""

print('\nData null customers:\n', customers_df.isnull().sum())
print('\nData null order items:\n', order_items.isnull().sum())
print('\nData null order payments:\n', order_pay.isnull().sum())
print('\nData null order reviews:\n', order_rev.isnull().sum())
print('\nData null orders:\n', orders_df.isnull().sum())
print('\nData null product category:\n', product_cat.isnull().sum())
print('\nData null products:\n', products_df.isnull().sum())
print('\nData null sellers:\n', sellers_df.isnull().sum())
print('\nData null geolocation:\n', geo_df.isnull().sum())

datetime_oo = ["order_purchase_timestamp","order_approved_at","order_delivered_carrier_date","order_delivered_customer_date","order_estimated_delivery_date"]
for column in datetime_oo:
  data['orders'][column] = pd.to_datetime(data['orders'][column])

datetime_oi = ["shipping_limit_date"]

for column in datetime_oi:
  data['items'][column] = pd.to_datetime(data['items'][column])

datetime_or = ["review_creation_date","review_answer_timestamp"]

for column in datetime_or:
  data['reviews'][column] = pd.to_datetime(data['reviews'][column])

"""Check Duplicates"""

# Initialize a dictionary to store duplicate counts and total sample counts
duplicate_counts = {}
total_sample_counts = {}

# Calculate and store the duplicate counts and total sample counts for each DataFrame
for df_name, df in data.items():
    duplicate_counts[df_name] = df.duplicated().sum()
    total_sample_counts[df_name] = len(df)  # Calculate the total number of samples

# Create DataFrames from the dictionaries
duplicate_counts_df = pd.DataFrame.from_dict(duplicate_counts, orient='index', columns=['Duplicate Count'])
total_sample_counts_df = pd.DataFrame.from_dict(total_sample_counts, orient='index', columns=['Total Sample Count'])

# Combine the two DataFrames by concatenating them horizontally
pd.concat([total_sample_counts_df, duplicate_counts_df], axis=1)

print('\nData describe customers:\n', customers_df.describe(include='all'))
print('\nData describe geolocation:\n', geo_df.describe(include='all'))
print('\nData describe order items:\n', order_items.describe(include='all'))
print('\nData describe order payments:\n', order_pay.describe(include='all'))
print('\nData describe order reviews:\n', order_rev.describe(include='all'))
print('\nData describe orders:\n', orders_df.describe(include='all'))
print('\nData describe product category:\n', product_cat.describe(include='all'))
print('\nData describe products:\n', products_df.describe(include='all'))
print('\nData describe sellers:\n', sellers_df.describe(include='all'))

"""*we can use this code*"""

print('Duplicates data customers:', customers_df.duplicated().sum())
print('Duplicates data order items:', order_items.duplicated().sum())
print('Duplicates data order payments:', order_pay.duplicated().sum())
print('Duplicates data order reviews:', order_rev.duplicated().sum())
print('Duplicates data orders:', orders_df.duplicated().sum())
print('Duplicates data product category:', product_cat.duplicated().sum())
print('Duplicates data products:', products_df.duplicated().sum())
print('Duplicates data sellers:', sellers_df.duplicated().sum())
print('Duplicates data geolocation:', geo_df.duplicated().sum())

"""Check Missing Values"""

# Initialize a dictionary to store null value counts for each DataFrame
null_counts = {}

# Calculate and store the null value counts for each DataFrame
for df_name, df in data.items():
    null_counts[df_name] = df.isna().sum()
    print(df_name, '\n', null_counts[df_name], '\n')

"""# **Data Cleaning**

There are duplicates in Geolocation Data Frames, we must clean before
"""

order_rev[order_rev.review_comment_title.isna()]

order_rev.review_comment_title.value_counts()

order_rev[order_rev.review_comment_message.isna()]

order_rev.review_comment_message.value_counts()

order_rev.fillna(value="no comment", inplace=True)

orders_df[orders_df.order_approved_at.isna()]

datetime_oi = ["shipping_limit_date"]

for column in datetime_oi:
  order_items[column] = pd.to_datetime(order_items[column])

datetime_or = ["review_creation_date","review_answer_timestamp"]

for column in datetime_or:
  order_rev[column] = pd.to_datetime(order_rev[column])

datetime_oo = ["order_purchase_timestamp","order_approved_at","order_delivered_carrier_date","order_delivered_customer_date","order_estimated_delivery_date"]

for column in datetime_oo:
  orders_df[column] = pd.to_datetime(orders_df[column])

order_items.info()

order_rev.info()

orders_df.info()

"""# **Exploratory Data Analysis (EDA)**

**Explore customers**
"""

data['customers'].sample(10)

data['customers'].groupby(by="customer_city").customer_id.nunique().sort_values(ascending=False)

data['customers'].groupby(by="customer_state").customer_id.nunique().sort_values(ascending=False)

"""**Explore payments**"""

data['payments'].sample(10)

data['payments'].groupby(by="payment_type").order_id.nunique().sort_values(ascending=False)

"""**Explore orders**"""

data['orders'].sample(10)

# I'm not particularly interested in the actual date of delivery, but more if it was delivered on time
data['orders']['delivered_on_time'] = np.where(data['orders']['order_delivered_customer_date'] < data['orders']['order_estimated_delivery_date'], 'On Time', 'Late')

data['orders']['delivered_on_time'].describe()

"""**Explore customers_df & orders_df**

Merge customers & orders
"""

customers_orders_df = pd.merge(
    left=data['customers'],
    right=data['orders'],
    how="left",
    left_on="customer_id",
    right_on="customer_id"
)
customers_orders_df.head()

"""Merge payments & reviews"""

payments_reviews_df = pd.merge(
    left=data['payments'],
    right=data['reviews'],
    how="left",
    left_on="order_id",
    right_on="order_id"
)
payments_reviews_df.head()

payments_reviews_df.sort_values(by="payment_value", ascending=False)

payments_reviews_df.groupby(by="payment_type").agg({
    "order_id": "nunique",
    "payment_value":  ["min", "max"]
})

"""Merge customers_orders & payments_reviews"""

customers_df = pd.merge(
    left=customers_orders_df,
    right=payments_reviews_df,
    how="left",
    left_on="order_id",
    right_on="order_id"
)
customers_df.head()

"""**Explore items & sellers**

Merge items & sellers
"""

item_seller_df = pd.merge(
    left=data['items'],
    right=data['sellers'],
    how="left",
    left_on="seller_id",
    right_on="seller_id"
)
item_seller_df.head()

item_seller_df.groupby(by="seller_city").seller_id.nunique().sort_values(ascending=False).head(10)

item_seller_df.groupby(by="seller_state").seller_id.nunique().sort_values(ascending=False).head(10)

"""** Explore products & category**

Merge products & category
"""

product_df = pd.merge(
    left=data['products'],
    right=data['category'],
    how="left",
    left_on="product_category_name",
    right_on="product_category_name"
)
product_df.head()

product_df.groupby(by="product_category_name").product_id.nunique().sort_values(ascending=False).head(10)

product_df.groupby(by="product_category_name_english").product_id.nunique().sort_values(ascending=False).head(10)

"""Merge item_seller_df & product_df"""

sellers_df = pd.merge(
    left=product_df,
    right=item_seller_df,
    how="left",
    left_on="product_id",
    right_on="product_id"
)
sellers_df.head()

sellers_df.sort_values(by="price", ascending=False)

sellers_df.groupby(by="product_category_name_english").agg({
    "order_id": "nunique",
    "price":  ["min", "max"]
})

"""**Explore geolocation**"""

data['geo'].sample(10)

data['geo'].groupby('geolocation_zip_code_prefix').size().sort_values(ascending=False)

data['geo'][data['geo']['geolocation_zip_code_prefix'] == 24230].head()

"""**Explore All Data**

Merge All Data
"""

all_data = pd.merge(
    left=customers_df,
    right=sellers_df,
    how="left",
    left_on="order_id",
    right_on="order_id"
)
all_data.head(20)

all_data_df = all_data.drop_duplicates('order_id')

all_data_df.info()

all_data.groupby(by="customer_state").agg({
    "order_id": "nunique",
    "payment_value": "sum"
}).sort_values(by="payment_value", ascending=False)

all_data.groupby(by="product_category_name_english").agg({
    "order_id": "nunique",
    "review_score":  ["min", "max"]
})

"""# **Convert All Data to CSV**"""

all_data.to_csv('all_data.csv', index=False)

"""# **Visualization & Explanatory Analysis**

# **Q1: What are the best-selling products and what are not?**
"""

sum_order_items_df = all_data.groupby("product_category_name_english")["product_id"].count().reset_index()
sum_order_items_df = sum_order_items_df.rename(columns={"product_id": "products"})
sum_order_items_df = sum_order_items_df.sort_values(by="products", ascending=False)
sum_order_items_df = sum_order_items_df.head(10)

sum_order_items_df.head()

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(24, 6))

colors = ["#068DA9", "#D3D3D3", "#D3D3D3", "#D3D3D3", "#D3D3D3"]

sns.barplot(x="products", y="product_category_name_english", data=sum_order_items_df.head(5), palette=colors, ax=ax[0])
ax[0].set_ylabel(None)
ax[0].set_xlabel(None)
ax[0].set_title("Most sold products", loc="center", fontsize=18)
ax[0].tick_params(axis ='y', labelsize=15)

sns.barplot(x="products", y="product_category_name_english", data=sum_order_items_df.sort_values(by="products", ascending=True).head(5), palette=colors, ax=ax[1])
ax[1].set_ylabel(None)
ax[1].set_xlabel(None)
ax[1].invert_xaxis()
ax[1].yaxis.set_label_position("right")
ax[1].yaxis.tick_right()
ax[1].set_title("Most fewest products", loc="center", fontsize=18)
ax[1].tick_params(axis='y', labelsize=15)

plt.suptitle("Best Selling Product", fontsize=20)

"""---
---
Based on the above graph, the most sold product is bed_bath_table, whereas the fewest product is auto

---
---

# **Q2: How much budget has the customer spent in the last few months?**
"""

monthly_df = all_data.resample(rule='M', on='order_approved_at').agg({
    "order_id": "nunique",
})
monthly_df.index = monthly_df.index.strftime('%B') #mengubah format order_approved_at menjadi Tahun-Bulan
monthly_df = monthly_df.reset_index()
monthly_df.rename(columns={
    "order_id": "order_count",
}, inplace=True)
monthly_df.head()

monthly_df = monthly_df.sort_values('order_count').drop_duplicates('order_approved_at', keep='last')

monthly_df.head()

monthly_df.sort_values(by='order_count')

month_mapping = {
    "January": 1,
    "February": 2,
    "March": 3,
    "April": 4,
    "May": 5,
    "June": 6,
    "July": 7,
    "August": 8,
    "September": 9,
    "October": 10,
    "November": 11,
    "December": 12
}

monthly_df["month_numeric"] = monthly_df["order_approved_at"].map(month_mapping)
monthly_df = monthly_df.sort_values("month_numeric")
monthly_df = monthly_df.drop("month_numeric", axis=1)

plt.figure(figsize=(10, 5))
plt.plot(
    monthly_df["order_approved_at"],
    monthly_df["order_count"],
    marker='o',
    linewidth=2,
    color="#068DA9"
)
plt.title("Number of Orders per Month (2023)", loc="center", fontsize=20)
plt.xticks(fontsize=10, rotation=25)
plt.yticks(fontsize=10)
plt.show()

"""---
---
Based on the above graph, it is seen in that there is a significant decrease in September and a significant increase in November

---
---

# **Q3 : How is the sales performance on the E-Commerce platform?**
"""

monthly_spend_df = all_data.resample(rule='M', on='order_approved_at').agg({
    "payment_value":"sum"
})
monthly_spend_df.index = monthly_spend_df.index.strftime('%B') #mengubah format order_approved_at menjadi Tahun-Bulan
monthly_spend_df = monthly_spend_df.reset_index()
monthly_spend_df.rename(columns={
    "payment_value":"total_spend"
}, inplace=True)
monthly_spend_df.head()

monthly_spend_df = monthly_spend_df.sort_values('total_spend').drop_duplicates('order_approved_at', keep='last')

monthly_spend_df.head()

monthly_spend_df.sort_values(by='total_spend')

monthly_spend_df["month_numeric"] = monthly_spend_df["order_approved_at"].map(month_mapping)
monthly_spend_df = monthly_spend_df.sort_values("month_numeric")
monthly_spend_df = monthly_spend_df.drop("month_numeric", axis=1)

plt.figure(figsize=(10, 5))
plt.plot(
    monthly_spend_df["order_approved_at"],
    monthly_spend_df["total_spend"],
    marker='o',
    linewidth=2,
    color="#068DA9"
)
plt.title("Total customer spend money per Month (2023)", loc="center", fontsize=20)
plt.xticks(fontsize=10, rotation=25)
plt.yticks(fontsize=10)
plt.show()

"""---
---
Based on the above graph, it is seen in that the total amount spent is the highest in November and the lowest in September.

---
---

# **Q4 : What is the level of customer satisfaction with our services?**
"""

review_scores = all_data['review_score'].value_counts().sort_values(ascending=False)

most_common_score = review_scores.idxmax()

sns.set(style="darkgrid")

plt.figure(figsize=(10, 5))
sns.barplot(x=review_scores.index,
            y=review_scores.values,
            order=review_scores.index,
            palette=["#068DA9" if score == most_common_score else "#D3D3D3" for score in review_scores.index]
            )

plt.title("Rating by customers for service", fontsize=15)
plt.xlabel("Rating")
plt.ylabel("Count")
plt.xticks(fontsize=12)
plt.show()

"""---
---
Based on the above graph, it is indicates that customers are highly satisfied with the provided services, as evidenced by the data showing that customers giving a rating of 5 have the highest count compared to other ratings

---
---

# **Q5: What are the demographic profiles of customers, and are there any differences in purchasing preferences among them?**
"""

bystate_df = all_data.groupby(by="customer_state").customer_id.nunique().reset_index()
bystate_df.rename(columns={
    "customer_id": "customer_count"
}, inplace=True)
bystate_df.head()

plt.figure(figsize=(12, 6))

most_common_state = bystate_df.loc[bystate_df['customer_count'].idxmax(), 'customer_state']

bystate_df = bystate_df.sort_values(by='customer_count', ascending=False)

sns.barplot(x='customer_state',
            y='customer_count',
            data=bystate_df,
            palette=["#068DA9" if state == most_common_state else "#D3D3D3" for state in bystate_df['customer_state']]
            )

plt.title("Number customers from State", fontsize=15)
plt.xlabel("State")
plt.ylabel("Number Customers")
plt.xticks(fontsize=10)
plt.show()

"""---
---
Based on the above graph, SP have the most customer from State

---
---
"""

bycity_df = all_data['customer_city'].value_counts().head(10)

plt.figure(figsize=(12, 6))

most_common_city = bycity_df.idxmax()

bycity_df = bycity_df.sort_values(ascending=False)

sns.barplot(x=bycity_df.index,
            y=bycity_df.values,
            palette=["#068DA9" if city == most_common_city else "#D3D3D3" for city in bycity_df.index]
            )

plt.title("Number of Customers from Each City", fontsize=15)
plt.xlabel("City")
plt.ylabel("Number of Customers")
plt.xticks(rotation=45, fontsize=10)
plt.show()

"""---
---
Based on the above graph, Sau Paulo have the most customer from each city

---
---
"""

order_status_counts = all_data['order_status'].value_counts()

plt.figure(figsize=(8, 6))
order_status_counts.plot(kind='bar', color='skyblue')
plt.title('Order Status')
plt.xlabel('Status')
plt.ylabel('Number of Orders')
plt.show()

"""---
---
Based on the above graph, the most order status is delivered
---
---

# **Q6: Based on geographical location, where are the majority of customers located?**
"""

other_state_geolocation = geo_df.groupby(['geolocation_zip_code_prefix'])['geolocation_state'].nunique().reset_index(name='count')
other_state_geolocation[other_state_geolocation['count']>= 2].shape
max_state = geo_df.groupby(['geolocation_zip_code_prefix','geolocation_state']).size().reset_index(name='count').drop_duplicates(subset = 'geolocation_zip_code_prefix').drop('count',axis=1)

geolocation_silver = geo_df.groupby(['geolocation_zip_code_prefix','geolocation_city','geolocation_state'])[['geolocation_lat','geolocation_lng']].median().reset_index()
geolocation_silver = geolocation_silver.merge(max_state,on=['geolocation_zip_code_prefix','geolocation_state'],how='inner')

customers_silver = customers_df.merge(geolocation_silver,left_on='customer_zip_code_prefix',right_on='geolocation_zip_code_prefix',how='inner')

customers_silver.head()

customers_silver.to_csv("geolocation.csv", index=False)

def plot_brazil_map(data):
    brazil = mpimg.imread(urllib.request.urlopen('https://i.pinimg.com/originals/3a/0c/e1/3a0ce18b3c842748c255bc0aa445ad41.jpg'),'jpg')
    ax = data.plot(kind="scatter", x="geolocation_lng", y="geolocation_lat", figsize=(10,10), alpha=0.3,s=0.3,c='maroon')
    plt.axis('off')
    plt.imshow(brazil, extent=[-73.98283055, -33.8,-33.75116944,5.4])
    plt.show()

plot_brazil_map(customers_silver.drop_duplicates(subset='customer_unique_id'))

"""---
---
Based on the above graph, the majority of customers are located in the southeastern and southern states
---
---

# **CONCLUSION**

1.   Based on the data and graph results, it shows that customers more frequently purchase bed_bath_table products, and the least purchased product is auto.
2.   According to the graph results, the E-Commerce sales performance is stable from January to May, with a slight decrease in June-July, a slight increase in August, and a significant decrease in September, followed by a very significant increase in October-November and a subsequent decrease in December.
3.   Based on the visualization results, as shown in the graph for question 2, the total amount spent by customers from January to May is stable, with a decrease in June-September, a significant increase in October-November, and a decrease again in December.
4.   Customer satisfaction with the provided services is very high because the visualization shows that customers giving a rating of 5 are very numerous, and those giving a rating of 4 are the second most numerous.
5.   The state with the most customers is SP, meaning the city with the most customers is Sao Paulo, and the second place is RJ (Rio de Janeiro). The most common order status for customer items is "delivered," indicating that there were no errors in the items ordered, and they were successfully delivered to the customers, as reflected in the customers giving a rating of 5 for E-Commerce services.
6.   Based on the graph, there are the most customers in the southeastern and southern regions. Additionally, there are more customers in capital cities (São Paulo, Rio de Janeiro, Porto Alegre, and others).
"""